{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYqTpb9A99eQ",
        "outputId": "48734074-d199-4237-f0c9-01d3f3edd723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting multilabel_knn\n",
            "  Downloading multilabel_knn-0.0.5-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from multilabel_knn) (1.21.6)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from multilabel_knn) (0.56.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from multilabel_knn) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from multilabel_knn) (1.0.2)\n",
            "Collecting faiss-gpu==1.6.5\n",
            "  Downloading faiss_gpu-1.6.5-cp38-cp38-manylinux2014_x86_64.whl (67.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 67.6 MB 72.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba->multilabel_knn) (4.13.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba->multilabel_knn) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba->multilabel_knn) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba->multilabel_knn) (3.11.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->multilabel_knn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->multilabel_knn) (1.2.0)\n",
            "Installing collected packages: faiss-gpu, multilabel-knn\n",
            "Successfully installed faiss-gpu-1.6.5 multilabel-knn-0.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install multilabel_knn\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils import data\n",
        "from torch.nn import functional as F\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as td\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from skimage.feature import hog as hog\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import plot_confusion_matrix,f1_score, accuracy_score, average_precision_score, recall_score\n",
        "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, LabelEncoder\n",
        "import multilabel_knn\n",
        "from skimage.feature import hog as hog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT3xhlMD-OoX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccd0e036-6e2d-4de3-8f37-be0f6f9454fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pathlib\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "drive = pathlib.Path('./drive/MyDrive') / 'ML_Project' / 'ML_FP_2022'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs9lnwy5Orre"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8KPir5l-SbM"
      },
      "outputs": [],
      "source": [
        "with open(drive / \"data2022\" / \"multi-task\" / \"label_v1\" / \"eval_label.pickle\", 'rb') as pickle_file:\n",
        "  eval_ = pickle.load(pickle_file)\n",
        "pickle_file.close()\n",
        "\n",
        "with open(drive / \"data2022\" / \"multi-task\" / \"label_v1\" / \"test_label.pickle\", 'rb') as pickle_file:\n",
        "  test_ = pickle.load(pickle_file)\n",
        "pickle_file.close()\n",
        "\n",
        "with open(drive / \"data2022\" / \"multi-task\" / \"label_v1\" / \"train_label.pickle\", 'rb') as pickle_file:\n",
        "  train_ = pickle.load(pickle_file)\n",
        "pickle_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXAjc2VuOuN2"
      },
      "source": [
        "# Define preprocessor and label encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW8_Pc2bFX9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d5f9d7-339e-4038-833d-ba22b7864150"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiLabelBinarizer()"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#data augmentation, define image preprocessor\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "   \n",
        "])\n",
        "\n",
        "labels_c = train_['creator']+test_['creator']+eval_['creator']\n",
        "encoder_c = LabelEncoder()\n",
        "encoder_c.fit(labels_c)\n",
        "\n",
        "labels_m = train_['material']+test_['material']+eval_['material']\n",
        "encoder_m = MultiLabelBinarizer()\n",
        "encoder_m.fit(labels_m)\n",
        "\n",
        "labels_t = train_['type']+test_['type']+eval_['type']\n",
        "encoder_t = MultiLabelBinarizer()\n",
        "encoder_t.fit(labels_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8wjL7GgSN4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9ea20a6-1839-44ac-86a1-9802fcc831a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Japans papier', 'dekverf', 'doek', 'ebbenhout', 'eikenhout',\n",
              "       'fluweel', 'goud', 'hout', 'inkt', 'koper', 'krijt', 'leer',\n",
              "       'olieverf', 'paneel', 'papier', 'porselein', 'potlood', 'staal',\n",
              "       'steengoed', 'verf', 'waterverf', 'zilver'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "encoder_m.classes_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sW6DHNqOmIW"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lnBxMs6Rrtl"
      },
      "outputs": [],
      "source": [
        "class train_dataset():\n",
        "  def __init__(self, dict_ = train_, encoder = [encoder_c, encoder_m, encoder_t]):\n",
        "    #load data\n",
        "    self.transform = preprocess\n",
        "    self.image = dict_['identifier']\n",
        "    self.creator = encoder[0].transform(dict_['creator'])\n",
        "    self.material = encoder[1].transform(dict_['material'])\n",
        "    self.art_type = encoder[2].transform(dict_['type'])\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.image)\n",
        "\n",
        "  def __getitem__(self, index, img_path=drive / \"data2022\" / \"multi-task\" / 'data' / \"train set new\"):\n",
        "    # img augmentation\n",
        "    img = cv2.imread(os.path.join(img_path / self.image[index]))\n",
        "    img = self.transform(img)\n",
        "    \n",
        "    #label\n",
        "    creator = self.creator[index]\n",
        "    material = self.material[index]\n",
        "    art_type = self.art_type[index]\n",
        "\n",
        "    sample = {'image' : img, 'creator' : creator, 'material' : material, 'type' : art_type}\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2g3YGPURwom"
      },
      "outputs": [],
      "source": [
        "class test_dataset():\n",
        "  def __init__(self, dict_ = test_, encoder = [encoder_c, encoder_m, encoder_t]):\n",
        "    #load data\n",
        "    self.transform = preprocess\n",
        "    self.image = dict_['identifier']\n",
        "    self.creator = encoder[0].transform(dict_['creator'])\n",
        "    self.material = encoder[1].transform(dict_['material'])\n",
        "    self.art_type = encoder[2].transform(dict_['type'])\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.image)\n",
        "\n",
        "  def __getitem__(self, index, img_path=drive / \"data2022\" / \"multi-task\" / 'data' / \"test set\"):\n",
        "    # img augmentation\n",
        "    img = cv2.imread(os.path.join(img_path / self.image[index]))\n",
        "    img = self.transform(img)\n",
        "    \n",
        "    #label\n",
        "    creator = self.creator[index]\n",
        "    material = self.material[index]\n",
        "    art_type = self.art_type[index]\n",
        "\n",
        "    sample = {'image' : img, 'creator' : creator, 'material' : material, 'type' : art_type}\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrKV5pyCKLzT"
      },
      "outputs": [],
      "source": [
        "class val_dataset():\n",
        "  def __init__(self, dict_ = eval_, encoder = [encoder_c, encoder_m, encoder_t]):\n",
        "    #load data\n",
        "    self.transform = preprocess\n",
        "    self.image = dict_['identifier']\n",
        "    self.creator = encoder[0].transform(dict_['creator'])\n",
        "    self.material = encoder[1].transform(dict_['material'])\n",
        "    self.art_type = encoder[2].transform(dict_['type'])\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.image)\n",
        "\n",
        "  def __getitem__(self, index, img_path=drive / \"data2022\" / \"multi-task\" / 'data' / \"eval set\"):\n",
        "    # img augmentation\n",
        "    img = cv2.imread(os.path.join(img_path / self.image[index]))\n",
        "    img = self.transform(img)\n",
        "    \n",
        "    #label\n",
        "    creator = self.creator[index]\n",
        "    material = self.material[index]\n",
        "    art_type = self.art_type[index]\n",
        "\n",
        "    sample = {'image' : img, 'creator' : creator, 'material' : material, 'type' : art_type}\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiFsxTmLPR0v"
      },
      "outputs": [],
      "source": [
        "train = train_dataset()\n",
        "test = test_dataset()\n",
        "val = val_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ViyLh4kO7pG"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE_wh-XiO-Kn"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataloader = DataLoader(train, shuffle=True, batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test, shuffle=False, batch_size=BATCH_SIZE)\n",
        "val_dataloader = DataLoader(val, shuffle=False, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-1LcLELUlF5"
      },
      "source": [
        "# Train and Evaluate Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw_dUa5aVNMB"
      },
      "outputs": [],
      "source": [
        "###main train and evaluate function ###\n",
        "def train_main(model,loss_fn,optimizer,train_loader,device):\n",
        "  loss_epoch=[]\n",
        "  for batchind,data in enumerate(train_loader):\n",
        "    model.train()\n",
        "    ##(b,c,h,w)\n",
        "    images = torch.Tensor(data['image']).to(device)\n",
        "    label = torch.Tensor(data['material']).to(device)\n",
        "    #print(images.shape)\n",
        "    optimizer.zero_grad()\n",
        "    outputs=model(images)\n",
        "    outputs=outputs.unsqueeze(1).float()\n",
        "    label=label.unsqueeze(1).float()\n",
        "\n",
        "    loss=loss_fn(outputs,label)\n",
        "    loss_epoch.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  return loss_epoch\n",
        "\n",
        "def evaluate_whole(model,evaluate_loader):\n",
        "  predict_list=[]\n",
        "  y_ = []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batchind,data in enumerate(evaluate_loader):\n",
        "      images = torch.Tensor(data['image']).to(device)\n",
        "      label = torch.Tensor(data['material']).to(device).float()\n",
        "      ##change to (1,c,h,w)\n",
        "      outputs=model(images).detach().float()\n",
        "      predict_list.append(torch.Tensor.round(outputs))\n",
        "      y_.append(torch.Tensor.round(label))\n",
        "  #mAP\n",
        "  print(y_[0].shape)\n",
        "  print(predict_list[0].shape)\n",
        "  mAp = 0\n",
        "  for index in range(len(predict_list)-1):\n",
        "    Ap = 0\n",
        "    for i in range(BATCH_SIZE):\n",
        "      Ap = Ap + average_precision_score(y_[index][i, :].detach().cpu().numpy(), predict_list[index][i, :].detach().cpu().numpy())\n",
        "    mAp = mAp + Ap / BATCH_SIZE\n",
        "  mAp = mAp/len(predict_list)\n",
        "  return predict_list, mAp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRovP-CdSc2H"
      },
      "source": [
        "# ResNet 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZsynjHiScGf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211,
          "referenced_widgets": [
            "47f16cc151ac4356a605bf2c1ea63c68",
            "d96feff01d2e41f08745ecfe7e09d363",
            "cd234985d19b40d9abf56a8e06681787",
            "f821275254194ed399fb599de074c233",
            "1ca9c1b9881b4cad84c9a22782aa8e2e",
            "0daf07ee061d46bdacabb5ca23aed3bf",
            "acd9ca1752ac4efabfc8327b8e5b3a8f",
            "89fcc75a57b2485dbcf6ee2bdb062347",
            "310e0aa222764e66ad949bcab7819031",
            "83d3b8805a7a457bb83fb782db173822",
            "c6eb84972ea842bb9b8a2d9bdfc2b417"
          ]
        },
        "outputId": "7fdab337-183e-46a5-d422-009a7ed87408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/hub.py:267: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/NVIDIA/DeepLearningExamples/zipball/torchhub\" to /root/.cache/torch/hub/torchhub.zip\n",
            "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
            "  warnings.warn(\n",
            "/root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
            "  warnings.warn(\n",
            "Downloading: \"https://api.ngc.nvidia.com/v2/models/nvidia/resnet50_pyt_amp/versions/20.06.0/files/nvidia_resnet50_200821.pth.tar\" to /root/.cache/torch/hub/checkpoints/nvidia_resnet50_200821.pth.tar\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47f16cc151ac4356a605bf2c1ea63c68"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "##load pretrained resnet50 network ###\n",
        "resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\n",
        "resnet50=nn.Sequential(\n",
        "    resnet50,\n",
        "    nn.Linear(1000,22),\n",
        "    nn.Sigmoid()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHEOySPRT1I3"
      },
      "outputs": [],
      "source": [
        "num_epoch=3\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=resnet50.to(device)\n",
        "model.load_state_dict(torch.load(drive / \"model\" / \"ResNet50_material_v2\" / 'model_paramenter_ResNet50_material_v2.txt'))\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer=optim.Adam(filter(lambda p:p.requires_grad,model.parameters()),lr=1e-4)\n",
        "state_dict = torch.load(drive / \"model\" / \"ResNet50_material_v2\" / 'optimizer_paramenter_ResNet50_material_v2.txt')\n",
        "optimizer.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss=[]\n",
        "evaluate_score=[]\n",
        "for i in range(num_epoch):\n",
        "  loss=train_main(model,loss_fn,optimizer,train_dataloader, device)\n",
        "  train_loss.append(np.mean(loss))\n",
        "  predict_r, mAp =evaluate_whole(model, evaluate_loader=val_dataloader)\n",
        "  evaluate_score.append(mAp)\n",
        "  print(f'epoch{i+1} has been trained')"
      ],
      "metadata": {
        "id": "sICYWAh0uwI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "X3STbamIvbrH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr_bjShutOfR",
        "outputId": "24ff6d48-ca5e-4910-a2e0-3efad3a06c4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 22])\n",
            "torch.Size([64, 22])\n"
          ]
        }
      ],
      "source": [
        "pr, mAp = evaluate_whole(model, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC4UeIwixkrf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b57266-f1b7-470f-b987-f7a1b9c26e5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9764194681186867"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "mAp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_list = []\n",
        "y_ = []\n",
        "for batchind,data in enumerate(test_dataloader):\n",
        "      label = torch.Tensor(data['material']).to(device).float()\n",
        "      predict_list.append(pr[batchind])\n",
        "      y_.append(torch.Tensor.round(label))\n",
        "  #mAP\n",
        "mRecall = 0\n",
        "for index in range(len(predict_list)-1):\n",
        "  recall = 0\n",
        "  for i in range(BATCH_SIZE):\n",
        "    recall = recall + recall_score(y_[index][i, :].detach().cpu().numpy(), predict_list[index][i, :].detach().cpu().numpy(), average='weighted')\n",
        "  mRecall = mRecall + recall / BATCH_SIZE\n",
        "mRecall = mRecall/len(predict_list)"
      ],
      "metadata": {
        "id": "myl2PKqnumN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mRecall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCNYk3kmxPnt",
        "outputId": "621df76c-9c08-4ca2-f23c-10a126f9f446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9788411458333335"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1 =  2*(mRecall*mAp)/(mRecall+mAp)\n",
        "f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZiCtQjgxXCC",
        "outputId": "a14d0928-b1ed-4560-a79d-6fe9c7b9b4dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9776288072979334"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1 =  2*(0.9321*0.9145)/(0.9321+0.9145)\n",
        "f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhRuuyitD9fC",
        "outputId": "3987d215-8fb5-469c-b380-70cd38a5110b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9232161269359904"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN"
      ],
      "metadata": {
        "id": "UI-cn9kisyDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_['creator']))\n",
        "print(len(test_['creator']))\n",
        "print(len(eval_['creator']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88ewtOY-zmfH",
        "outputId": "3be7f284-b899-46d1-f884-2c6bb1d7b222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10682\n",
            "3053\n",
            "1526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlk = multilabel_knn.multilabel_kNN(k=22, metric = \"cosine\")\n",
        "mlk.fit(hog_train, train)\n",
        "Y_pred = model.predict(X_test)\n",
        "Y_prob = model.predict(X_test, return_prob = True)"
      ],
      "metadata": {
        "id": "6VkuV5nd5dd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-MIzm3KJvlS"
      },
      "source": [
        "."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "47f16cc151ac4356a605bf2c1ea63c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d96feff01d2e41f08745ecfe7e09d363",
              "IPY_MODEL_cd234985d19b40d9abf56a8e06681787",
              "IPY_MODEL_f821275254194ed399fb599de074c233"
            ],
            "layout": "IPY_MODEL_1ca9c1b9881b4cad84c9a22782aa8e2e"
          }
        },
        "d96feff01d2e41f08745ecfe7e09d363": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0daf07ee061d46bdacabb5ca23aed3bf",
            "placeholder": "​",
            "style": "IPY_MODEL_acd9ca1752ac4efabfc8327b8e5b3a8f",
            "value": "100%"
          }
        },
        "cd234985d19b40d9abf56a8e06681787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89fcc75a57b2485dbcf6ee2bdb062347",
            "max": 102491118,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_310e0aa222764e66ad949bcab7819031",
            "value": 102491118
          }
        },
        "f821275254194ed399fb599de074c233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83d3b8805a7a457bb83fb782db173822",
            "placeholder": "​",
            "style": "IPY_MODEL_c6eb84972ea842bb9b8a2d9bdfc2b417",
            "value": " 97.7M/97.7M [00:04&lt;00:00, 37.1MB/s]"
          }
        },
        "1ca9c1b9881b4cad84c9a22782aa8e2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0daf07ee061d46bdacabb5ca23aed3bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acd9ca1752ac4efabfc8327b8e5b3a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89fcc75a57b2485dbcf6ee2bdb062347": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "310e0aa222764e66ad949bcab7819031": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83d3b8805a7a457bb83fb782db173822": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6eb84972ea842bb9b8a2d9bdfc2b417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}